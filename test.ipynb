{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75da47b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 14:10:31.193629: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-21 14:10:31.195819: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-21 14:10:31.236453: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-21 14:10:31.878015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache\"\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import SwinModel\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from transformers.models.swin.modeling_swin import (\n",
    "    SwinLayer,\n",
    "    SwinSelfAttention,\n",
    "    SwinSelfOutput,\n",
    "    SwinIntermediate,\n",
    "    SwinOutput\n",
    ")\n",
    "from transformers import SwinConfig\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7219aa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, config, dim, num_heads, window_size)\n",
      "(self, image_size=224, patch_size=4, num_channels=3, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7, mlp_ratio=4.0, qkv_bias=True, hidden_dropout_prob=0.0, attention_probs_dropout_prob=0.0, drop_path_rate=0.1, hidden_act='gelu', use_absolute_embeddings=False, initializer_range=0.02, layer_norm_eps=1e-05, encoder_stride=32, out_features=None, out_indices=None, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.signature(SwinSelfAttention.__init__))\n",
    "print(inspect.signature(SwinConfig.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b69b2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de17222",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"ISIC2018_Task1-2_Training_Input\"\n",
    "MASK_PATH = \"ISIC2018_Task1_Training_GroundTruth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18da2e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(8, 0)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # 预期输出 True\n",
    "print(torch.cuda.get_device_capability(0))  # SM 版本需 ≥ 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b19ceb",
   "metadata": {},
   "source": [
    "## Model Constructing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999fee5a",
   "metadata": {},
   "source": [
    "### Test now(2025.5.19) still in test but can run now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839faed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size, H, W):\n",
    "    B, L, C = x.shape\n",
    "    print(f'B: {B}, L: {L}, C: {C}')\n",
    "    x = x.view(B, H, W, C)\n",
    "    x = x.unfold(1, window_size, window_size).unfold(2, window_size, window_size)\n",
    "    print(f\"x_after unfold: {x.shape}\")\n",
    "    x = x.contiguous().view(-1, window_size*window_size, C)\n",
    "    print(f\"x_after view: {x.shape}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16caef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, H, W, B):\n",
    "    num_windows = (H // window_size) * (W // window_size)\n",
    "    x = windows.view(B, num_windows, window_size * window_size, -1)\n",
    "    print(f\"x_after view: {x.shape}\")\n",
    "    x = x.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    print(f\"x_after view: {x.shape}\")\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    print(f\"x_after permute: {x.shape}\")\n",
    "    return x.view(B, H*W, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0756d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.swin.modeling_swin import SwinSelfAttention, SwinSelfOutput, SwinIntermediate, SwinOutput\n",
    "import torch\n",
    "\n",
    "class SimpleSwinBlock(torch.nn.Module):\n",
    "    def __init__(self, config, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.self_output = SwinSelfOutput(config, dim)\n",
    "        self.intermediate = SwinIntermediate(config, dim)\n",
    "        self.output = SwinOutput(config, dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "        self.norm2 = torch.nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "        # 将注意力层作为模块的一个属性\n",
    "        self.attn = None\n",
    "\n",
    "    def get_attn(self, window_size):\n",
    "        # 动态创建或更新注意力层\n",
    "        if self.attn is None or self.attn.window_size != window_size:\n",
    "            self.attn = SwinSelfAttention(self.config, self.dim, self.num_heads, window_size)\n",
    "        return self.attn\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        max_window = 7\n",
    "        window_size = min(max_window, H, W)\n",
    "        for ws in range(window_size, 0, -1):\n",
    "            if H % ws == 0 and W % ws == 0:\n",
    "                window_size = ws\n",
    "                break\n",
    "        print(f'H: {H}, W: {W}, window_size: {window_size}')\n",
    "        \n",
    "        # 获取适合当前分辨率的注意力层\n",
    "        attn = self.get_attn(window_size)\n",
    "        \n",
    "        shortcut = x\n",
    "        B = shortcut.shape[0]\n",
    "        print(f'shortcut.shape: {shortcut.shape}')\n",
    "        # (batch_size, seq_len, dim)\n",
    "        x = self.norm1(x)\n",
    "        print(f\"self.norm1(x).shape: {x.shape}\")\n",
    "        # (batch_size, seq_len, dim)\n",
    "        x_windows = window_partition(x, window_size, H, W)\n",
    "        print(f\"x_windows.shape: {x_windows.shape}\")\n",
    "        x_windows = attn(x_windows)\n",
    "        if isinstance(x_windows, tuple):\n",
    "            x_windows = x_windows[0]\n",
    "        x = window_reverse(x_windows, window_size, H, W, B)\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        # (batch_size, seq_len, dim)\n",
    "        x = self.self_output(x, shortcut)\n",
    "        print(f\"self_output.shape: {x.shape}\")\n",
    "        # (batch_size, seq_len, dim)\n",
    "        shortcut2 = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.intermediate(x)\n",
    "        print(f\"intermediate.shape: {x.shape}\")\n",
    "        #(batch_size, seq_len, 4*dim)\n",
    "        x = self.output(x)\n",
    "        print(f\"output.shape: {x.shape}\")\n",
    "        x = x + shortcut2\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        #(batch_size, seq_len, dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa05342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpand(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, scale=2):\n",
    "        super().__init__()\n",
    "        self.proj = torch.nn.Linear(input_dim, output_dim * scale * scale)\n",
    "        self.scale = scale\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "        H = W = int(L ** 0.5)\n",
    "        x = self.proj(x)  # (B, L, output_dim*scale*scale)\n",
    "        x = x.view(B, H, W, self.scale, self.scale, self.output_dim)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        # (B, H, scale, W, scale, output_dim)\n",
    "        x = x.view(B, H * self.scale, W * self.scale, self.output_dim)\n",
    "        return x.view(B, -1, self.output_dim)\n",
    "        # (B, L*scale*scale, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a3edb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerDecoder(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 对应编码器输出的通道和分辨率\n",
    "        self.dims = [768, 384, 192, 96]\n",
    "        self.heads = [32, 16, 8, 4]\n",
    "        self.config = config\n",
    "\n",
    "        self.up_blocks = torch.nn.ModuleList()\n",
    "        for i in range(3):\n",
    "            self.up_blocks.append(PatchExpand(self.dims[i], self.dims[i+1], scale=2))\n",
    "            self.up_blocks.append(torch.nn.Linear(self.dims[i+1]*2, self.dims[i+1]))  # 融合skip\n",
    "            self.up_blocks.append(SimpleSwinBlock(config, self.dims[i+1], self.heads[i+1]))\n",
    "\n",
    "        self.final_proj = torch.nn.Linear(self.dims[-1], 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[0]  # 7x7x768\n",
    "        skips = features[1:]  # [14x14x384, 28x28x192, 56x56x96]\n",
    "        for i in range(3):\n",
    "            x = self.up_blocks[i*3](x)  # PatchExpand\n",
    "            skip = skips[i]\n",
    "            # 上采样skip到x的空间分辨率\n",
    "            if x.shape[1] != skip.shape[1]:\n",
    "                B, L_skip, C_skip = skip.shape\n",
    "                H_skip = W_skip = int(L_skip ** 0.5)\n",
    "                H_x = W_x = int(x.shape[1] ** 0.5)\n",
    "                skip_ = skip.view(B, H_skip, W_skip, C_skip).permute(0, 3, 1, 2)\n",
    "                skip_ = torch.nn.functional.interpolate(skip_, size=(H_x, W_x), mode='bilinear', align_corners=False)\n",
    "                skip = skip_.permute(0, 2, 3, 1).reshape(B, H_x * W_x, C_skip)\n",
    "            x = torch.cat([x, skip], dim=-1)\n",
    "            x = self.up_blocks[i*3+1](x)\n",
    "            B, L, C = x.shape\n",
    "            H = W = int(L ** 0.5)\n",
    "            x = self.up_blocks[i*3+2](x, H, W)\n",
    "        # 后续同前\n",
    "        B, L, C = x.shape\n",
    "        H = W = int(L ** 0.5)\n",
    "        x = x.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, -1, C)\n",
    "        x = self.final_proj(x)\n",
    "        x = x.view(B, 224, 224, 1).permute(0, 3, 1, 2)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c72234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SwinModel, SwinConfig\n",
    "\n",
    "class SwinUNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", output_hidden_states=True)\n",
    "        self.decoder = SwinTransformerDecoder(self.encoder.config)\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if \"layers.2\" not in name and \"layers.3\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.encoder(x)\n",
    "        hs = outputs.hidden_states\n",
    "        print(f'hs[0].shape: {hs[0].shape}')\n",
    "        print(f'hs[1].shape: {hs[1].shape}')\n",
    "        print(f'hs[2].shape: {hs[2].shape}')\n",
    "        print(f'hs[3].shape: {hs[3].shape}')\n",
    "        print(f'hs[4].shape: {hs[4].shape}')\n",
    "        # [7x7x768, 14x14x384, 28x28x192, 56x56x96]\n",
    "        features = []\n",
    "        for i in [3, 2, 1, 0]:\n",
    "            feat = hs[i]\n",
    "            B, L, C = feat.shape\n",
    "            H = W = int(L ** 0.5)\n",
    "            features.append(feat.view(B, H, W, C).reshape(B, -1, C))\n",
    "        return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77d4f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs[0].shape: torch.Size([2, 3136, 96])\n",
      "hs[1].shape: torch.Size([2, 784, 192])\n",
      "hs[2].shape: torch.Size([2, 196, 384])\n",
      "hs[3].shape: torch.Size([2, 49, 768])\n",
      "hs[4].shape: torch.Size([2, 49, 768])\n",
      "H: 14, W: 14, window_size: 7\n",
      "shortcut.shape: torch.Size([2, 196, 384])\n",
      "self.norm1(x).shape: torch.Size([2, 196, 384])\n",
      "B: 2, L: 196, C: 384\n",
      "x_after unfold: torch.Size([2, 2, 2, 384, 7, 7])\n",
      "x_after view: torch.Size([8, 49, 384])\n",
      "x_windows.shape: torch.Size([8, 49, 384])\n",
      "x_after view: torch.Size([2, 4, 49, 384])\n",
      "x_after view: torch.Size([2, 2, 2, 7, 7, 384])\n",
      "x_after permute: torch.Size([2, 14, 14, 384])\n",
      "x.shape: torch.Size([2, 196, 384])\n",
      "self_output.shape: torch.Size([2, 196, 384])\n",
      "intermediate.shape: torch.Size([2, 196, 1536])\n",
      "output.shape: torch.Size([2, 196, 384])\n",
      "x.shape: torch.Size([2, 196, 384])\n",
      "H: 28, W: 28, window_size: 7\n",
      "shortcut.shape: torch.Size([2, 784, 192])\n",
      "self.norm1(x).shape: torch.Size([2, 784, 192])\n",
      "B: 2, L: 784, C: 192\n",
      "x_after unfold: torch.Size([2, 4, 4, 192, 7, 7])\n",
      "x_after view: torch.Size([32, 49, 192])\n",
      "x_windows.shape: torch.Size([32, 49, 192])\n",
      "x_after view: torch.Size([2, 16, 49, 192])\n",
      "x_after view: torch.Size([2, 4, 4, 7, 7, 192])\n",
      "x_after permute: torch.Size([2, 28, 28, 192])\n",
      "x.shape: torch.Size([2, 784, 192])\n",
      "self_output.shape: torch.Size([2, 784, 192])\n",
      "intermediate.shape: torch.Size([2, 784, 768])\n",
      "output.shape: torch.Size([2, 784, 192])\n",
      "x.shape: torch.Size([2, 784, 192])\n",
      "H: 56, W: 56, window_size: 7\n",
      "shortcut.shape: torch.Size([2, 3136, 96])\n",
      "self.norm1(x).shape: torch.Size([2, 3136, 96])\n",
      "B: 2, L: 3136, C: 96\n",
      "x_after unfold: torch.Size([2, 8, 8, 96, 7, 7])\n",
      "x_after view: torch.Size([128, 49, 96])\n",
      "x_windows.shape: torch.Size([128, 49, 96])\n",
      "x_after view: torch.Size([2, 64, 49, 96])\n",
      "x_after view: torch.Size([2, 8, 8, 7, 7, 96])\n",
      "x_after permute: torch.Size([2, 56, 56, 96])\n",
      "x.shape: torch.Size([2, 3136, 96])\n",
      "self_output.shape: torch.Size([2, 3136, 96])\n",
      "intermediate.shape: torch.Size([2, 3136, 384])\n",
      "output.shape: torch.Size([2, 3136, 96])\n",
      "x.shape: torch.Size([2, 3136, 96])\n",
      "输出shape: torch.Size([2, 1, 224, 224])\n",
      "encoder.embeddings.patch_embeddings.projection.weight\n",
      "encoder.embeddings.patch_embeddings.projection.bias\n",
      "encoder.embeddings.norm.weight\n",
      "encoder.embeddings.norm.bias\n",
      "encoder.encoder.layers.0.blocks.0.layernorm_before.weight\n",
      "encoder.encoder.layers.0.blocks.0.layernorm_before.bias\n",
      "encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.0.blocks.0.attention.self.query.weight\n",
      "encoder.encoder.layers.0.blocks.0.attention.self.query.bias\n",
      "encoder.encoder.layers.0.blocks.0.attention.self.key.weight\n",
      "encoder.encoder.layers.0.blocks.0.attention.self.key.bias\n",
      "encoder.encoder.layers.0.blocks.0.attention.self.value.weight\n",
      "encoder.encoder.layers.0.blocks.0.attention.self.value.bias\n",
      "encoder.encoder.layers.0.blocks.0.attention.output.dense.weight\n",
      "encoder.encoder.layers.0.blocks.0.attention.output.dense.bias\n",
      "encoder.encoder.layers.0.blocks.0.layernorm_after.weight\n",
      "encoder.encoder.layers.0.blocks.0.layernorm_after.bias\n",
      "encoder.encoder.layers.0.blocks.0.intermediate.dense.weight\n",
      "encoder.encoder.layers.0.blocks.0.intermediate.dense.bias\n",
      "encoder.encoder.layers.0.blocks.0.output.dense.weight\n",
      "encoder.encoder.layers.0.blocks.0.output.dense.bias\n",
      "encoder.encoder.layers.0.blocks.1.layernorm_before.weight\n",
      "encoder.encoder.layers.0.blocks.1.layernorm_before.bias\n",
      "encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.0.blocks.1.attention.self.query.weight\n",
      "encoder.encoder.layers.0.blocks.1.attention.self.query.bias\n",
      "encoder.encoder.layers.0.blocks.1.attention.self.key.weight\n",
      "encoder.encoder.layers.0.blocks.1.attention.self.key.bias\n",
      "encoder.encoder.layers.0.blocks.1.attention.self.value.weight\n",
      "encoder.encoder.layers.0.blocks.1.attention.self.value.bias\n",
      "encoder.encoder.layers.0.blocks.1.attention.output.dense.weight\n",
      "encoder.encoder.layers.0.blocks.1.attention.output.dense.bias\n",
      "encoder.encoder.layers.0.blocks.1.layernorm_after.weight\n",
      "encoder.encoder.layers.0.blocks.1.layernorm_after.bias\n",
      "encoder.encoder.layers.0.blocks.1.intermediate.dense.weight\n",
      "encoder.encoder.layers.0.blocks.1.intermediate.dense.bias\n",
      "encoder.encoder.layers.0.blocks.1.output.dense.weight\n",
      "encoder.encoder.layers.0.blocks.1.output.dense.bias\n",
      "encoder.encoder.layers.0.downsample.reduction.weight\n",
      "encoder.encoder.layers.0.downsample.norm.weight\n",
      "encoder.encoder.layers.0.downsample.norm.bias\n",
      "encoder.encoder.layers.1.blocks.0.layernorm_before.weight\n",
      "encoder.encoder.layers.1.blocks.0.layernorm_before.bias\n",
      "encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.1.blocks.0.attention.self.query.weight\n",
      "encoder.encoder.layers.1.blocks.0.attention.self.query.bias\n",
      "encoder.encoder.layers.1.blocks.0.attention.self.key.weight\n",
      "encoder.encoder.layers.1.blocks.0.attention.self.key.bias\n",
      "encoder.encoder.layers.1.blocks.0.attention.self.value.weight\n",
      "encoder.encoder.layers.1.blocks.0.attention.self.value.bias\n",
      "encoder.encoder.layers.1.blocks.0.attention.output.dense.weight\n",
      "encoder.encoder.layers.1.blocks.0.attention.output.dense.bias\n",
      "encoder.encoder.layers.1.blocks.0.layernorm_after.weight\n",
      "encoder.encoder.layers.1.blocks.0.layernorm_after.bias\n",
      "encoder.encoder.layers.1.blocks.0.intermediate.dense.weight\n",
      "encoder.encoder.layers.1.blocks.0.intermediate.dense.bias\n",
      "encoder.encoder.layers.1.blocks.0.output.dense.weight\n",
      "encoder.encoder.layers.1.blocks.0.output.dense.bias\n",
      "encoder.encoder.layers.1.blocks.1.layernorm_before.weight\n",
      "encoder.encoder.layers.1.blocks.1.layernorm_before.bias\n",
      "encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.1.blocks.1.attention.self.query.weight\n",
      "encoder.encoder.layers.1.blocks.1.attention.self.query.bias\n",
      "encoder.encoder.layers.1.blocks.1.attention.self.key.weight\n",
      "encoder.encoder.layers.1.blocks.1.attention.self.key.bias\n",
      "encoder.encoder.layers.1.blocks.1.attention.self.value.weight\n",
      "encoder.encoder.layers.1.blocks.1.attention.self.value.bias\n",
      "encoder.encoder.layers.1.blocks.1.attention.output.dense.weight\n",
      "encoder.encoder.layers.1.blocks.1.attention.output.dense.bias\n",
      "encoder.encoder.layers.1.blocks.1.layernorm_after.weight\n",
      "encoder.encoder.layers.1.blocks.1.layernorm_after.bias\n",
      "encoder.encoder.layers.1.blocks.1.intermediate.dense.weight\n",
      "encoder.encoder.layers.1.blocks.1.intermediate.dense.bias\n",
      "encoder.encoder.layers.1.blocks.1.output.dense.weight\n",
      "encoder.encoder.layers.1.blocks.1.output.dense.bias\n",
      "encoder.encoder.layers.1.downsample.reduction.weight\n",
      "encoder.encoder.layers.1.downsample.norm.weight\n",
      "encoder.encoder.layers.1.downsample.norm.bias\n",
      "encoder.encoder.layers.2.blocks.0.layernorm_before.weight\n",
      "encoder.encoder.layers.2.blocks.0.layernorm_before.bias\n",
      "encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.2.blocks.0.attention.self.query.weight\n",
      "encoder.encoder.layers.2.blocks.0.attention.self.query.bias\n",
      "encoder.encoder.layers.2.blocks.0.attention.self.key.weight\n",
      "encoder.encoder.layers.2.blocks.0.attention.self.key.bias\n",
      "encoder.encoder.layers.2.blocks.0.attention.self.value.weight\n",
      "encoder.encoder.layers.2.blocks.0.attention.self.value.bias\n",
      "encoder.encoder.layers.2.blocks.0.attention.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.0.attention.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.0.layernorm_after.weight\n",
      "encoder.encoder.layers.2.blocks.0.layernorm_after.bias\n",
      "encoder.encoder.layers.2.blocks.0.intermediate.dense.weight\n",
      "encoder.encoder.layers.2.blocks.0.intermediate.dense.bias\n",
      "encoder.encoder.layers.2.blocks.0.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.0.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.1.layernorm_before.weight\n",
      "encoder.encoder.layers.2.blocks.1.layernorm_before.bias\n",
      "encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.2.blocks.1.attention.self.query.weight\n",
      "encoder.encoder.layers.2.blocks.1.attention.self.query.bias\n",
      "encoder.encoder.layers.2.blocks.1.attention.self.key.weight\n",
      "encoder.encoder.layers.2.blocks.1.attention.self.key.bias\n",
      "encoder.encoder.layers.2.blocks.1.attention.self.value.weight\n",
      "encoder.encoder.layers.2.blocks.1.attention.self.value.bias\n",
      "encoder.encoder.layers.2.blocks.1.attention.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.1.attention.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.1.layernorm_after.weight\n",
      "encoder.encoder.layers.2.blocks.1.layernorm_after.bias\n",
      "encoder.encoder.layers.2.blocks.1.intermediate.dense.weight\n",
      "encoder.encoder.layers.2.blocks.1.intermediate.dense.bias\n",
      "encoder.encoder.layers.2.blocks.1.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.1.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.2.layernorm_before.weight\n",
      "encoder.encoder.layers.2.blocks.2.layernorm_before.bias\n",
      "encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.2.blocks.2.attention.self.query.weight\n",
      "encoder.encoder.layers.2.blocks.2.attention.self.query.bias\n",
      "encoder.encoder.layers.2.blocks.2.attention.self.key.weight\n",
      "encoder.encoder.layers.2.blocks.2.attention.self.key.bias\n",
      "encoder.encoder.layers.2.blocks.2.attention.self.value.weight\n",
      "encoder.encoder.layers.2.blocks.2.attention.self.value.bias\n",
      "encoder.encoder.layers.2.blocks.2.attention.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.2.attention.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.2.layernorm_after.weight\n",
      "encoder.encoder.layers.2.blocks.2.layernorm_after.bias\n",
      "encoder.encoder.layers.2.blocks.2.intermediate.dense.weight\n",
      "encoder.encoder.layers.2.blocks.2.intermediate.dense.bias\n",
      "encoder.encoder.layers.2.blocks.2.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.2.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.3.layernorm_before.weight\n",
      "encoder.encoder.layers.2.blocks.3.layernorm_before.bias\n",
      "encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.2.blocks.3.attention.self.query.weight\n",
      "encoder.encoder.layers.2.blocks.3.attention.self.query.bias\n",
      "encoder.encoder.layers.2.blocks.3.attention.self.key.weight\n",
      "encoder.encoder.layers.2.blocks.3.attention.self.key.bias\n",
      "encoder.encoder.layers.2.blocks.3.attention.self.value.weight\n",
      "encoder.encoder.layers.2.blocks.3.attention.self.value.bias\n",
      "encoder.encoder.layers.2.blocks.3.attention.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.3.attention.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.3.layernorm_after.weight\n",
      "encoder.encoder.layers.2.blocks.3.layernorm_after.bias\n",
      "encoder.encoder.layers.2.blocks.3.intermediate.dense.weight\n",
      "encoder.encoder.layers.2.blocks.3.intermediate.dense.bias\n",
      "encoder.encoder.layers.2.blocks.3.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.3.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.4.layernorm_before.weight\n",
      "encoder.encoder.layers.2.blocks.4.layernorm_before.bias\n",
      "encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.2.blocks.4.attention.self.query.weight\n",
      "encoder.encoder.layers.2.blocks.4.attention.self.query.bias\n",
      "encoder.encoder.layers.2.blocks.4.attention.self.key.weight\n",
      "encoder.encoder.layers.2.blocks.4.attention.self.key.bias\n",
      "encoder.encoder.layers.2.blocks.4.attention.self.value.weight\n",
      "encoder.encoder.layers.2.blocks.4.attention.self.value.bias\n",
      "encoder.encoder.layers.2.blocks.4.attention.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.4.attention.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.4.layernorm_after.weight\n",
      "encoder.encoder.layers.2.blocks.4.layernorm_after.bias\n",
      "encoder.encoder.layers.2.blocks.4.intermediate.dense.weight\n",
      "encoder.encoder.layers.2.blocks.4.intermediate.dense.bias\n",
      "encoder.encoder.layers.2.blocks.4.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.4.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.5.layernorm_before.weight\n",
      "encoder.encoder.layers.2.blocks.5.layernorm_before.bias\n",
      "encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.2.blocks.5.attention.self.query.weight\n",
      "encoder.encoder.layers.2.blocks.5.attention.self.query.bias\n",
      "encoder.encoder.layers.2.blocks.5.attention.self.key.weight\n",
      "encoder.encoder.layers.2.blocks.5.attention.self.key.bias\n",
      "encoder.encoder.layers.2.blocks.5.attention.self.value.weight\n",
      "encoder.encoder.layers.2.blocks.5.attention.self.value.bias\n",
      "encoder.encoder.layers.2.blocks.5.attention.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.5.attention.output.dense.bias\n",
      "encoder.encoder.layers.2.blocks.5.layernorm_after.weight\n",
      "encoder.encoder.layers.2.blocks.5.layernorm_after.bias\n",
      "encoder.encoder.layers.2.blocks.5.intermediate.dense.weight\n",
      "encoder.encoder.layers.2.blocks.5.intermediate.dense.bias\n",
      "encoder.encoder.layers.2.blocks.5.output.dense.weight\n",
      "encoder.encoder.layers.2.blocks.5.output.dense.bias\n",
      "encoder.encoder.layers.2.downsample.reduction.weight\n",
      "encoder.encoder.layers.2.downsample.norm.weight\n",
      "encoder.encoder.layers.2.downsample.norm.bias\n",
      "encoder.encoder.layers.3.blocks.0.layernorm_before.weight\n",
      "encoder.encoder.layers.3.blocks.0.layernorm_before.bias\n",
      "encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.3.blocks.0.attention.self.query.weight\n",
      "encoder.encoder.layers.3.blocks.0.attention.self.query.bias\n",
      "encoder.encoder.layers.3.blocks.0.attention.self.key.weight\n",
      "encoder.encoder.layers.3.blocks.0.attention.self.key.bias\n",
      "encoder.encoder.layers.3.blocks.0.attention.self.value.weight\n",
      "encoder.encoder.layers.3.blocks.0.attention.self.value.bias\n",
      "encoder.encoder.layers.3.blocks.0.attention.output.dense.weight\n",
      "encoder.encoder.layers.3.blocks.0.attention.output.dense.bias\n",
      "encoder.encoder.layers.3.blocks.0.layernorm_after.weight\n",
      "encoder.encoder.layers.3.blocks.0.layernorm_after.bias\n",
      "encoder.encoder.layers.3.blocks.0.intermediate.dense.weight\n",
      "encoder.encoder.layers.3.blocks.0.intermediate.dense.bias\n",
      "encoder.encoder.layers.3.blocks.0.output.dense.weight\n",
      "encoder.encoder.layers.3.blocks.0.output.dense.bias\n",
      "encoder.encoder.layers.3.blocks.1.layernorm_before.weight\n",
      "encoder.encoder.layers.3.blocks.1.layernorm_before.bias\n",
      "encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.encoder.layers.3.blocks.1.attention.self.query.weight\n",
      "encoder.encoder.layers.3.blocks.1.attention.self.query.bias\n",
      "encoder.encoder.layers.3.blocks.1.attention.self.key.weight\n",
      "encoder.encoder.layers.3.blocks.1.attention.self.key.bias\n",
      "encoder.encoder.layers.3.blocks.1.attention.self.value.weight\n",
      "encoder.encoder.layers.3.blocks.1.attention.self.value.bias\n",
      "encoder.encoder.layers.3.blocks.1.attention.output.dense.weight\n",
      "encoder.encoder.layers.3.blocks.1.attention.output.dense.bias\n",
      "encoder.encoder.layers.3.blocks.1.layernorm_after.weight\n",
      "encoder.encoder.layers.3.blocks.1.layernorm_after.bias\n",
      "encoder.encoder.layers.3.blocks.1.intermediate.dense.weight\n",
      "encoder.encoder.layers.3.blocks.1.intermediate.dense.bias\n",
      "encoder.encoder.layers.3.blocks.1.output.dense.weight\n",
      "encoder.encoder.layers.3.blocks.1.output.dense.bias\n",
      "encoder.layernorm.weight\n",
      "encoder.layernorm.bias\n",
      "decoder.up_blocks.0.proj.weight\n",
      "decoder.up_blocks.0.proj.bias\n",
      "decoder.up_blocks.1.weight\n",
      "decoder.up_blocks.1.bias\n",
      "decoder.up_blocks.2.self_output.dense.weight\n",
      "decoder.up_blocks.2.self_output.dense.bias\n",
      "decoder.up_blocks.2.intermediate.dense.weight\n",
      "decoder.up_blocks.2.intermediate.dense.bias\n",
      "decoder.up_blocks.2.output.dense.weight\n",
      "decoder.up_blocks.2.output.dense.bias\n",
      "decoder.up_blocks.2.norm1.weight\n",
      "decoder.up_blocks.2.norm1.bias\n",
      "decoder.up_blocks.2.norm2.weight\n",
      "decoder.up_blocks.2.norm2.bias\n",
      "decoder.up_blocks.2.attn.relative_position_bias_table\n",
      "decoder.up_blocks.2.attn.query.weight\n",
      "decoder.up_blocks.2.attn.query.bias\n",
      "decoder.up_blocks.2.attn.key.weight\n",
      "decoder.up_blocks.2.attn.key.bias\n",
      "decoder.up_blocks.2.attn.value.weight\n",
      "decoder.up_blocks.2.attn.value.bias\n",
      "decoder.up_blocks.3.proj.weight\n",
      "decoder.up_blocks.3.proj.bias\n",
      "decoder.up_blocks.4.weight\n",
      "decoder.up_blocks.4.bias\n",
      "decoder.up_blocks.5.self_output.dense.weight\n",
      "decoder.up_blocks.5.self_output.dense.bias\n",
      "decoder.up_blocks.5.intermediate.dense.weight\n",
      "decoder.up_blocks.5.intermediate.dense.bias\n",
      "decoder.up_blocks.5.output.dense.weight\n",
      "decoder.up_blocks.5.output.dense.bias\n",
      "decoder.up_blocks.5.norm1.weight\n",
      "decoder.up_blocks.5.norm1.bias\n",
      "decoder.up_blocks.5.norm2.weight\n",
      "decoder.up_blocks.5.norm2.bias\n",
      "decoder.up_blocks.5.attn.relative_position_bias_table\n",
      "decoder.up_blocks.5.attn.query.weight\n",
      "decoder.up_blocks.5.attn.query.bias\n",
      "decoder.up_blocks.5.attn.key.weight\n",
      "decoder.up_blocks.5.attn.key.bias\n",
      "decoder.up_blocks.5.attn.value.weight\n",
      "decoder.up_blocks.5.attn.value.bias\n",
      "decoder.up_blocks.6.proj.weight\n",
      "decoder.up_blocks.6.proj.bias\n",
      "decoder.up_blocks.7.weight\n",
      "decoder.up_blocks.7.bias\n",
      "decoder.up_blocks.8.self_output.dense.weight\n",
      "decoder.up_blocks.8.self_output.dense.bias\n",
      "decoder.up_blocks.8.intermediate.dense.weight\n",
      "decoder.up_blocks.8.intermediate.dense.bias\n",
      "decoder.up_blocks.8.output.dense.weight\n",
      "decoder.up_blocks.8.output.dense.bias\n",
      "decoder.up_blocks.8.norm1.weight\n",
      "decoder.up_blocks.8.norm1.bias\n",
      "decoder.up_blocks.8.norm2.weight\n",
      "decoder.up_blocks.8.norm2.bias\n",
      "decoder.up_blocks.8.attn.relative_position_bias_table\n",
      "decoder.up_blocks.8.attn.query.weight\n",
      "decoder.up_blocks.8.attn.query.bias\n",
      "decoder.up_blocks.8.attn.key.weight\n",
      "decoder.up_blocks.8.attn.key.bias\n",
      "decoder.up_blocks.8.attn.value.weight\n",
      "decoder.up_blocks.8.attn.value.bias\n",
      "decoder.final_proj.weight\n",
      "decoder.final_proj.bias\n"
     ]
    }
   ],
   "source": [
    "model = SwinUNet()\n",
    "dummy_input = torch.randn(2, 3, 224, 224)\n",
    "output = model(dummy_input)\n",
    "print('输出shape:', output.shape)  # 应为 [2, 1, 224, 224]\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3248f441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients exist: 149 / 263\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss()(output, torch.rand(2, 1, 224, 224))\n",
    "loss.backward()\n",
    "grad_exists = [p.grad is not None for p in model.parameters()]\n",
    "print(\"Gradients exist:\", sum(grad_exists), \"/\", len(grad_exists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SIZE = 224\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f28f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = SwinUNetDecoder()\n",
    "\n",
    "# dummy_features = [\n",
    "#     # torch.randn(1, 768, 7, 7),    # Stage4\n",
    "#     torch.randn(1, 384, 14, 14),  # Stage3\n",
    "#     torch.randn(1, 192, 28, 28),  # Stage2\n",
    "#     torch.randn(1, 96, 56, 56)    # Stage1\n",
    "# ]\n",
    "\n",
    "# output = decoder(dummy_features)\n",
    "# print(f\"Final output shape: {output.shape}\")  # 应输出 [1,1,224,224]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f04c1c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__ (self, image_dir, mask_dir, size, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.ids = [image_dir[:-4] for image_dir in os.listdir(image_dir) if image_dir.endswith('.jpg')]\n",
    "        self.size = size\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.ids[idx] + \".jpg\")\n",
    "        mask_path = os.path.join(self.mask_dir, self.ids[idx] + \"_segmentation.png\")\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            seed = torch.random.seed()\n",
    "            torch.random.manual_seed(seed)\n",
    "            img = self.transform(img)\n",
    "            torch.random.manual_seed(seed)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        mask = np.array(mask)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        mask = (mask > 127).float() # 二值化处理\n",
    "\n",
    "        img = transforms.functional.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add1096",
   "metadata": {},
   "source": [
    "#### Loading and Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ISICDataset(image_dir=TRAIN_PATH, mask_dir=MASK_PATH, size=SIZE, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = Subset(dataset, range(train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, len(dataset)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadb413",
   "metadata": {},
   "source": [
    "## Build the training process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
