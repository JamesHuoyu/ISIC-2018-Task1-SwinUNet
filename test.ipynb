{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75da47b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 19:44:12.155484: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 19:44:12.157647: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-19 19:44:12.197600: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-19 19:44:12.817831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache\"\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import SwinModel\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from transformers.models.swin.modeling_swin import (\n",
    "    SwinLayer,\n",
    "    SwinSelfAttention,\n",
    "    SwinSelfOutput,\n",
    "    SwinIntermediate,\n",
    "    SwinOutput\n",
    ")\n",
    "from transformers import SwinConfig\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7219aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.signature(SwinSelfAttention.__init__))\n",
    "print(inspect.signature(SwinConfig.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b69b2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de17222",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"ISIC2018_Task1-2_Training_Input\"\n",
    "MASK_PATH = \"ISIC2018_Task1_Training_GroundTruth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b19ceb",
   "metadata": {},
   "source": [
    "## Model Constructing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999fee5a",
   "metadata": {},
   "source": [
    "### Test now(2025.5.19) still in test but can run now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "839faed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size, H, W):\n",
    "    B, L, C = x.shape\n",
    "    print(f'B: {B}, L: {L}, C: {C}')\n",
    "    x = x.view(B, H, W, C)\n",
    "    x = x.unfold(1, window_size, window_size).unfold(2, window_size, window_size)\n",
    "    print(f\"x_after unfold: {x.shape}\")\n",
    "    x = x.contiguous().view(-1, window_size*window_size, C)\n",
    "    print(f\"x_after view: {x.shape}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e16caef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, H, W, B):\n",
    "    num_windows = (H // window_size) * (W // window_size)\n",
    "    x = windows.view(B, num_windows, window_size * window_size, -1)\n",
    "    print(f\"x_after view: {x.shape}\")\n",
    "    x = x.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
    "    print(f\"x_after view: {x.shape}\")\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    print(f\"x_after permute: {x.shape}\")\n",
    "    return x.view(B, H*W, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0756d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.swin.modeling_swin import SwinSelfAttention, SwinSelfOutput, SwinIntermediate, SwinOutput\n",
    "import torch\n",
    "\n",
    "class SimpleSwinBlock(torch.nn.Module):\n",
    "    def __init__(self, config, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        # self.attn = SwinSelfAttention(config, dim, num_heads, window_size)\n",
    "        self.self_output = SwinSelfOutput(config, dim)\n",
    "        self.intermediate = SwinIntermediate(config, dim)\n",
    "        self.output = SwinOutput(config, dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "        self.norm2 = torch.nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        max_window = 7\n",
    "        window_size = min(max_window, H, W)\n",
    "        for ws in range(window_size, 0, -1):\n",
    "            if H % ws == 0 and W % ws == 0:\n",
    "                window_size = ws\n",
    "                break\n",
    "        print(f'H: {H}, W: {W}, window_size: {window_size}')\n",
    "        attn = SwinSelfAttention(self.config, self.dim, self.num_heads, window_size)\n",
    "        shortcut = x\n",
    "        B = shortcut.shape[0]\n",
    "        print(f'shortcut.shape: {shortcut.shape}')\n",
    "        # (batch_size, seq_len, dim)\n",
    "        x = self.norm1(x)\n",
    "        print(f\"self.norm1(x).shape: {x.shape}\")\n",
    "        # (batch_size, seq_len, dim)\n",
    "        x_windows = window_partition(x, window_size, H, W)\n",
    "        print(f\"x_windows.shape: {x_windows.shape}\")\n",
    "        x_windows = attn(x_windows)\n",
    "        if isinstance(x_windows, tuple):\n",
    "            x_windows = x_windows[0]\n",
    "        x = window_reverse(x_windows, window_size, H, W, B)\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        # (batch_size, seq_len, dim)\n",
    "        x = self.self_output(x, shortcut)\n",
    "        print(f\"self_output.shape: {x.shape}\")\n",
    "        # (batch_size, seq_len, dim)\n",
    "        shortcut2 = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.intermediate(x)\n",
    "        print(f\"intermediate.shape: {x.shape}\")\n",
    "        #(batch_size, seq_len, 4*dim)\n",
    "        x = self.output(x)\n",
    "        print(f\"output.shape: {x.shape}\")\n",
    "        x = x + shortcut2\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        #(batch_size, seq_len, dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa05342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpand(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, scale=2):\n",
    "        super().__init__()\n",
    "        self.proj = torch.nn.Linear(input_dim, output_dim * scale * scale)\n",
    "        self.scale = scale\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "        H = W = int(L ** 0.5)\n",
    "        x = self.proj(x)  # (B, L, output_dim*scale*scale)\n",
    "        x = x.view(B, H, W, self.scale, self.scale, self.output_dim)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        # (B, H, scale, W, scale, output_dim)\n",
    "        x = x.view(B, H * self.scale, W * self.scale, self.output_dim)\n",
    "        return x.view(B, -1, self.output_dim)\n",
    "        # (B, L*scale*scale, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a3edb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerDecoder(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 对应编码器输出的通道和分辨率\n",
    "        self.dims = [768, 384, 192, 96]\n",
    "        self.heads = [32, 16, 8, 4]\n",
    "        self.config = config\n",
    "\n",
    "        self.up_blocks = torch.nn.ModuleList()\n",
    "        for i in range(3):\n",
    "            self.up_blocks.append(PatchExpand(self.dims[i], self.dims[i+1], scale=2))\n",
    "            self.up_blocks.append(torch.nn.Linear(self.dims[i+1]*2, self.dims[i+1]))  # 融合skip\n",
    "            self.up_blocks.append(SimpleSwinBlock(config, self.dims[i+1], self.heads[i+1]))\n",
    "\n",
    "        self.final_proj = torch.nn.Linear(self.dims[-1], 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[0]  # 7x7x768\n",
    "        skips = features[1:]  # [14x14x384, 28x28x192, 56x56x96]\n",
    "        for i in range(3):\n",
    "            x = self.up_blocks[i*3](x)  # PatchExpand\n",
    "            skip = skips[i]\n",
    "            # 上采样skip到x的空间分辨率\n",
    "            if x.shape[1] != skip.shape[1]:\n",
    "                B, L_skip, C_skip = skip.shape\n",
    "                H_skip = W_skip = int(L_skip ** 0.5)\n",
    "                H_x = W_x = int(x.shape[1] ** 0.5)\n",
    "                skip_ = skip.view(B, H_skip, W_skip, C_skip).permute(0, 3, 1, 2)\n",
    "                skip_ = torch.nn.functional.interpolate(skip_, size=(H_x, W_x), mode='bilinear', align_corners=False)\n",
    "                skip = skip_.permute(0, 2, 3, 1).reshape(B, H_x * W_x, C_skip)\n",
    "            x = torch.cat([x, skip], dim=-1)\n",
    "            x = self.up_blocks[i*3+1](x)\n",
    "            B, L, C = x.shape\n",
    "            H = W = int(L ** 0.5)\n",
    "            x = self.up_blocks[i*3+2](x, H, W)\n",
    "        # 后续同前\n",
    "        B, L, C = x.shape\n",
    "        H = W = int(L ** 0.5)\n",
    "        x = x.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, -1, C)\n",
    "        x = self.final_proj(x)\n",
    "        x = x.view(B, 224, 224, 1).permute(0, 3, 1, 2)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37c72234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SwinModel, SwinConfig\n",
    "\n",
    "class SwinUNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", output_hidden_states=True)\n",
    "        self.decoder = SwinTransformerDecoder(self.encoder.config)\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if \"layers.2\" not in name and \"layers.3\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.encoder(x)\n",
    "        hs = outputs.hidden_states\n",
    "        print(f'hs[0].shape: {hs[0].shape}')\n",
    "        print(f'hs[1].shape: {hs[1].shape}')\n",
    "        print(f'hs[2].shape: {hs[2].shape}')\n",
    "        print(f'hs[3].shape: {hs[3].shape}')\n",
    "        print(f'hs[4].shape: {hs[4].shape}')\n",
    "        # [7x7x768, 14x14x384, 28x28x192, 56x56x96]\n",
    "        features = []\n",
    "        for i in [3, 2, 1, 0]:\n",
    "            feat = hs[i]\n",
    "            B, L, C = feat.shape\n",
    "            H = W = int(L ** 0.5)\n",
    "            features.append(feat.view(B, H, W, C).reshape(B, -1, C))\n",
    "        return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f77d4f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs[0].shape: torch.Size([2, 3136, 96])\n",
      "hs[1].shape: torch.Size([2, 784, 192])\n",
      "hs[2].shape: torch.Size([2, 196, 384])\n",
      "hs[3].shape: torch.Size([2, 49, 768])\n",
      "hs[4].shape: torch.Size([2, 49, 768])\n",
      "H: 14, W: 14, window_size: 7\n",
      "shortcut.shape: torch.Size([2, 196, 384])\n",
      "self.norm1(x).shape: torch.Size([2, 196, 384])\n",
      "B: 2, L: 196, C: 384\n",
      "x_after unfold: torch.Size([2, 2, 2, 384, 7, 7])\n",
      "x_after view: torch.Size([8, 49, 384])\n",
      "x_windows.shape: torch.Size([8, 49, 384])\n",
      "x_after view: torch.Size([2, 4, 49, 384])\n",
      "x_after view: torch.Size([2, 2, 2, 7, 7, 384])\n",
      "x_after permute: torch.Size([2, 14, 14, 384])\n",
      "x.shape: torch.Size([2, 196, 384])\n",
      "self_output.shape: torch.Size([2, 196, 384])\n",
      "intermediate.shape: torch.Size([2, 196, 1536])\n",
      "output.shape: torch.Size([2, 196, 384])\n",
      "x.shape: torch.Size([2, 196, 384])\n",
      "H: 28, W: 28, window_size: 7\n",
      "shortcut.shape: torch.Size([2, 784, 192])\n",
      "self.norm1(x).shape: torch.Size([2, 784, 192])\n",
      "B: 2, L: 784, C: 192\n",
      "x_after unfold: torch.Size([2, 4, 4, 192, 7, 7])\n",
      "x_after view: torch.Size([32, 49, 192])\n",
      "x_windows.shape: torch.Size([32, 49, 192])\n",
      "x_after view: torch.Size([2, 16, 49, 192])\n",
      "x_after view: torch.Size([2, 4, 4, 7, 7, 192])\n",
      "x_after permute: torch.Size([2, 28, 28, 192])\n",
      "x.shape: torch.Size([2, 784, 192])\n",
      "self_output.shape: torch.Size([2, 784, 192])\n",
      "intermediate.shape: torch.Size([2, 784, 768])\n",
      "output.shape: torch.Size([2, 784, 192])\n",
      "x.shape: torch.Size([2, 784, 192])\n",
      "H: 56, W: 56, window_size: 7\n",
      "shortcut.shape: torch.Size([2, 3136, 96])\n",
      "self.norm1(x).shape: torch.Size([2, 3136, 96])\n",
      "B: 2, L: 3136, C: 96\n",
      "x_after unfold: torch.Size([2, 8, 8, 96, 7, 7])\n",
      "x_after view: torch.Size([128, 49, 96])\n",
      "x_windows.shape: torch.Size([128, 49, 96])\n",
      "x_after view: torch.Size([2, 64, 49, 96])\n",
      "x_after view: torch.Size([2, 8, 8, 7, 7, 96])\n",
      "x_after permute: torch.Size([2, 56, 56, 96])\n",
      "x.shape: torch.Size([2, 3136, 96])\n",
      "self_output.shape: torch.Size([2, 3136, 96])\n",
      "intermediate.shape: torch.Size([2, 3136, 384])\n",
      "output.shape: torch.Size([2, 3136, 96])\n",
      "x.shape: torch.Size([2, 3136, 96])\n",
      "输出shape: torch.Size([2, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "model = SwinUNet()\n",
    "dummy_input = torch.randn(2, 3, 224, 224)\n",
    "output = model(dummy_input)\n",
    "print('输出shape:', output.shape)  # 应为 [2, 1, 224, 224]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b1ebd",
   "metadata": {},
   "source": [
    "### Old attempt(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb70fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpand(torch.nn.Module):\n",
    "    def __init__(self, dim, dim_scale=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.expand = torch.nn.Linear(dim, dim*dim_scale**2, bias=False)\n",
    "        self.dim_scale = dim_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: (B, L, C)\n",
    "        output: (B, L, C//4)\n",
    "        '''\n",
    "        B, L, C = x.shape\n",
    "        H = W = int(L**0.5)\n",
    "        x = self.expand(x)  # (B,L,4C)\n",
    "        x = x.view(B, H, W, C*self.dim_scale**2)\n",
    "        x = x.reshape(B, H*self.dim_scale, W*self.dim_scale, C)\n",
    "        return x.reshape(B, (H*self.dim_scale)**2, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "177badef",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = SwinConfig(\n",
    "    image_size = 224,\n",
    "    patch_size = 4,\n",
    "    depths = [2, 2, 18, 2],\n",
    "    num_heads = [4, 8, 16, 32],\n",
    "    embed_dim = 96\n",
    ")\n",
    "\n",
    "class DecoderConfig(SwinConfig):\n",
    "    def __init__(\n",
    "            self,\n",
    "            decoder_depths:list,\n",
    "            hidden_size:int,\n",
    "            encoder_resolution:list,\n",
    "            qkv_bias:bool = True,\n",
    "            attn_drop_rate:float = 0.0,\n",
    "            proj_drop_rate:float = 0.0,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            proj_drop_rate=proj_drop_rate,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.decoder_depths = decoder_depths\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder_resolution = encoder_resolution\n",
    "\n",
    "decoder_config = DecoderConfig(\n",
    "    decoder_depths=[2, 2, 2, 2],\n",
    "    hidden_size=96,\n",
    "    encoder_resolution=[\n",
    "        (96, (56, 56)),\n",
    "        (192, (28, 28)),\n",
    "        (384, (14, 14)),\n",
    "        (768, (7, 7))\n",
    "    ],\n",
    "    qkv_bias=True,\n",
    "    attn_drop_rate=0.1,\n",
    "    proj_drop_rate=0.1,\n",
    "    num_heads=[32, 16, 8, 4],\n",
    "    window_size=7,\n",
    "    embed_dim=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d3a12f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: DecoderConfig, dim, input_resolution, num_heads, shift_size=0):\n",
    "        super().__init__()\n",
    "        self.attention = SwinSelfAttention(\n",
    "            config, dim, num_heads, window_size=7\n",
    "        )\n",
    "        self.self_output = SwinSelfOutput(config, dim)\n",
    "        self.intermediate = SwinIntermediate(config, dim)\n",
    "        self.output = SwinOutput(config, dim)\n",
    "        self.layernorm_before = torch.nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after = torch.nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        input = hidden_states\n",
    "        hidden_states = self.layernorm_before(hidden_states)\n",
    "        attention_output = self.attention(hidden_states)\n",
    "        attention_output = self.self_output(attention_output, input)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        layer_output = self.layernorm_after(layer_output + input)\n",
    "        return layer_output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "546682e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerUNetDecoder(torch.nn.Module):\n",
    "    def __init__(self, config: DecoderConfig, encoder_resolutions):\n",
    "        super().__init__()\n",
    "        self.stages = torch.nn.ModuleList()\n",
    "        for i, (dim, (h, w)) in enumerate(reversed(encoder_resolutions)):\n",
    "            # Upsample\n",
    "            if i != 0:\n",
    "                self.stages.append(PatchExpand(dim, dim_scale=2))\n",
    "            \n",
    "            stage = torch.nn.Sequential(\n",
    "                *[SwinTransformerBlock(\n",
    "                    config,\n",
    "                    dim=dim,\n",
    "                    input_resolution=(h*(2**i), w*(2**i)),\n",
    "                    num_heads=config.num_heads[i]\n",
    "                ) for _ in range(config.depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "\n",
    "        self.final = torch.nn.Conv2d(config.hidden_size, 1, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x, skips):\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            if isinstance(stage, PatchExpand):\n",
    "                x = stage(x)\n",
    "                \n",
    "                x = torch.cat([x, skips[i]], dim=-1)\n",
    "            else:\n",
    "                x = stage(x)\n",
    "        B, L, C = x.shape\n",
    "        H = W = int(L**0.5)\n",
    "        x = x.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return torch.sigmoid(self.final(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f17ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinUNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "        self.decoder = TransformerUNetDecoder(decoder_config, decoder_config.encoder_resolutions)\n",
    "\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if \"layers.3\" not in name:\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder_outputs = self.encoder(x, output_hidden_states=True).hidden_states\n",
    "        skips = list(reversed(encoder_outputs[:-1]))\n",
    "\n",
    "        return self.decoder(encoder_outputs[-1], skips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e6773fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinUNet()\n",
    "dummy_input = torch.randn(2, 3, 224, 224)\n",
    "output = model(dummy_input)\n",
    "print(output.shape)  # 应输出 torch.Size([2, 1, 224, 224])\n",
    "\n",
    "# 梯度检查\n",
    "loss = torch.nn.BCELoss()(output, torch.rand(2,1,224,224))\n",
    "loss.backward()\n",
    "print([p.grad is not None for p in model.parameters()])  # 应显示部分梯度存在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SIZE = 224\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f28f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = SwinUNetDecoder()\n",
    "\n",
    "# dummy_features = [\n",
    "#     # torch.randn(1, 768, 7, 7),    # Stage4\n",
    "#     torch.randn(1, 384, 14, 14),  # Stage3\n",
    "#     torch.randn(1, 192, 28, 28),  # Stage2\n",
    "#     torch.randn(1, 96, 56, 56)    # Stage1\n",
    "# ]\n",
    "\n",
    "# output = decoder(dummy_features)\n",
    "# print(f\"Final output shape: {output.shape}\")  # 应输出 [1,1,224,224]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f04c1c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__ (self, image_dir, mask_dir, size, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.ids = [image_dir[:-4] for image_dir in os.listdir(image_dir) if image_dir.endswith('.jpg')]\n",
    "        self.size = size\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.ids[idx] + \".jpg\")\n",
    "        mask_path = os.path.join(self.mask_dir, self.ids[idx] + \"_segmentation.png\")\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            seed = torch.random.seed()\n",
    "            torch.random.manual_seed(seed)\n",
    "            img = self.transform(img)\n",
    "            torch.random.manual_seed(seed)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        mask = np.array(mask)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        mask = (mask > 127).float() # 二值化处理\n",
    "\n",
    "        img = transforms.functional.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add1096",
   "metadata": {},
   "source": [
    "#### Loading and Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ISICDataset(image_dir=TRAIN_PATH, mask_dir=MASK_PATH, size=SIZE, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = Subset(dataset, range(train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, len(dataset)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadb413",
   "metadata": {},
   "source": [
    "## Build the training process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
